{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-28T20:54:52.516342Z",
     "start_time": "2024-02-28T20:54:50.108465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "stopwords = set(stopwords.words('russian'))\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def to_tokens(text):\n",
    "    no_punct = re.sub('[!,.:;?]', '', text)\n",
    "    pymorphy_text = list(map(lambda x: morph.parse(x)[0].normal_form, no_punct.split() ))\n",
    "    text_no_stopwords = [word for word in pymorphy_text if word not in stopwords]\n",
    "    return text_no_stopwords\n",
    "\n",
    "def get_grams_intersection(res, vac):\n",
    "    \n",
    "    tokens_resume = to_tokens(res)\n",
    "    tokens_vacancy = to_tokens(vac)\n",
    "    print(tokens_resume[:100])\n",
    "    \n",
    "    bigram_resume = set(nltk.ngrams(tokens_resume, 3))\n",
    "    bigram_vacancy = set(nltk.ngrams(tokens_vacancy, 3))\n",
    "    \n",
    "    intersection = bigram_resume & bigram_vacancy\n",
    "    return intersection\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.1\n",
      "0.0 0.0\n",
      "0.0 0.1\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.3\n",
      "0.0 0.1\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "1.0 0.5\n",
      "0.0 0.9\n",
      "1.0 0.5\n",
      "0.0 0.0\n",
      "0.0 0.1\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "5.0 0.9\n",
      "0.0 0.2\n",
      "0.0 0.2\n",
      "0.0 0.3\n",
      "0.0 0.1\n",
      "0.0 0.0\n",
      "2.0 0.2\n",
      "0.0 0.0\n",
      "0.0 0.1\n",
      "0.0 0.2\n",
      "0.0 0.4\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.16551724137931034\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "cur_dir = os.getcwd()\n",
    "file_name = cur_dir + '/case_2_data_for_members.json'\n",
    "with open(file_name, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "differences = []\n",
    "\n",
    "for job in data:\n",
    "    failed_job_intersections = []\n",
    "    success_job_intersections = []\n",
    "    resume = job['vacancy']['description']\n",
    "    tokens_resume = to_tokens(resume)\n",
    "    unigram_resume = set(nltk.ngrams(tokens_resume, 3))\n",
    "    \n",
    "    for failed in job['failed_resumes']:\n",
    "        if 'experienceItem' in failed:\n",
    "            unigram_experience = set()\n",
    "            for last_job in failed['experienceItem']:\n",
    "                \n",
    "                if last_job['description'] is not None:\n",
    "                    experience = last_job['description']\n",
    "                    tokens_experience = to_tokens(experience)\n",
    "                    unigram_experience = unigram_experience | set(nltk.ngrams(tokens_experience, 3))\n",
    "                    \n",
    "                    \n",
    "            failed_job_intersections.append(len(unigram_experience & unigram_resume))\n",
    "\n",
    "    for success in job['confirmed_resumes']:\n",
    "        if 'experienceItem' in success:\n",
    "            unigram_experience = set()\n",
    "            for last_job in failed['experienceItem']:\n",
    "\n",
    "                if last_job['description'] is not None:\n",
    "                    experience = last_job['description']\n",
    "                    tokens_experience = to_tokens(experience)\n",
    "                    unigram_experience = unigram_experience | set(nltk.ngrams(tokens_experience, 3))\n",
    "\n",
    "\n",
    "            success_job_intersections.append(len(unigram_experience & unigram_resume))\n",
    "    \n",
    "    avg_suc = round(sum(success_job_intersections) / len(success_job_intersections), 1)\n",
    "    avg_failed = round(sum(failed_job_intersections) / len(failed_job_intersections), 1)\n",
    "    differences.append( avg_suc -  avg_failed)\n",
    "    #print(success_job_intersections)\n",
    "    print(avg_suc, avg_failed)\n",
    "                    \n",
    "                    \n",
    "print(sum(differences) / len(differences))                    \n",
    "                   \n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T21:26:42.534883Z",
     "start_time": "2024-02-28T21:26:17.576767Z"
    }
   },
   "id": "d9032340e4266fce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нет зависимости между общими парами слов в резюме/акансии\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c1db4b3308c9377"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'java-разработчик', 'java-developer'}\n",
      "{'Git', 'SpringBoot', 'JavaSpringFramework', 'ApacheKafka', 'Hibernate', 'REST', 'PostgreSQL', 'Java', 'Camunda'}\n",
      "{'Maven', 'Boot', 'jtapi', 'POI', 'Security', 'Spring', 'Github', 'Tomcat', 'Bitbucket', 'to', 'code', 'review', 'Kafka', 'JasperReports', 'market', 'Mockito', 'EJB', 'Docker', 'Swagger', 'legacy', 'Liquibase', 'time', 'jUnit', 'Avaya', 'Data', 'WildFly', 'call', 'Postgres', 'CRM', 'API'}\n",
      "74\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#вроде норм тема, 95% извлекает правильно\n",
    "def extract_english_words(text):\n",
    "    english_word_pattern = re.compile(r'\\b[a-zA-Z]+\\b')\n",
    "    english_words = english_word_pattern.findall(text)\n",
    "\n",
    "    return english_words\n",
    "\n",
    "#возвращает кем работал, key_skills, extra_skills, сколько работал (в месяцах), возраст\n",
    "def get_workin_years_extra_skills_positions(resume):\n",
    "    positions = set()\n",
    "    key_skills = set(resume['key_skills'].replace('.', '').replace(' ', '').split(',')) if 'key_skills' in resume else set()\n",
    "    extra_skills = set()\n",
    "    working_months = 0\n",
    "    years = 2024 - int(resume['birth_date'].split('-')[0])\n",
    "\n",
    "\n",
    "    about = resume['about'] if 'description' in resume and  resume['about'] is not None  else ''\n",
    "    extra_skills = set(extract_english_words(about))\n",
    "    \n",
    "    if 'experienceItem' in resume and resume['experienceItem'] is not None:\n",
    "        for last_job in resume['experienceItem']:\n",
    "            if 'position' in last_job and last_job['position'] is not None:\n",
    "                positions.add( last_job['position'])\n",
    "            \n",
    "            if last_job['starts'] != None:\n",
    "                start = list(map(int, last_job['starts'].split('-')))\n",
    "                ends = list(map(int, last_job['ends'].split('-'))) if last_job['ends'] != None else [2024]\n",
    "        \n",
    "                if len(start) < 2:\n",
    "                    start.append(0)\n",
    "\n",
    "                if len(ends) < 2:\n",
    "                    ends.append(0)\n",
    "        \n",
    "            \n",
    "                months = (ends[0] - start[0]) * 12\n",
    "                months += ends[1] - start[1]        \n",
    "                working_months += months\n",
    "                \n",
    "            descript = last_job['description'] if 'description' in last_job and last_job['description'] is not None else ''\n",
    "            \n",
    "            #print(descript)\n",
    "            skills = set(extract_english_words(descript))\n",
    "            \n",
    "            extra_skills = skills | extra_skills\n",
    "    extra_skills -= key_skills\n",
    "    return positions, key_skills, extra_skills, working_months, years\n",
    "\n",
    "test = data[0]['confirmed_resumes'][0]\n",
    "\n",
    "for item in (get_workin_years_extra_skills_positions(test)):\n",
    "    print(item)\n",
    "                \n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T22:45:08.081163Z",
     "start_time": "2024-02-28T22:45:08.078250Z"
    }
   },
   "id": "a659920d35e42655"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7995c7233223224"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
