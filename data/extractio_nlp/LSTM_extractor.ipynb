{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GjnGu2gmXqa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import array\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from sklearn import preprocessing\n",
        "import tqdm\n",
        "import time\n",
        "import torch\n",
        "import copy\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import scipy.spatial.distance as ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "zsKZdETUyzTy"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/case_2_data_for_members.json') as f:\n",
        "    file_content = f.read()\n",
        "    data = json.loads(file_content)\n",
        "\n",
        "\n",
        "vector_of_vacancy = []\n",
        "for i in range(len(data)-1):\n",
        "  vector_of_vacancy.append(data[i]['vacancy'])\n",
        "\n",
        "descriptions = []\n",
        "for i in vector_of_vacancy:\n",
        "  descriptions.append(i['description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WEnLmF-4AhgY"
      },
      "outputs": [],
      "source": [
        "#parce_employeers\n",
        "\n",
        "empl = []\n",
        "\n",
        "for i in range(len(data)-1):\n",
        "\n",
        "    for j in data[i][\"failed_resumes\"]:\n",
        "      s = ''\n",
        "      if (\"about\" in j):\n",
        "        if j['about'] is not None:\n",
        "          s+=j['about'] + \" \"\n",
        "      if ('experienceItem' in j):\n",
        "        for k in j['experienceItem']:\n",
        "          if ('description' in k):\n",
        "            if k['description'] is not None:\n",
        "              s += k['description'] + \" \"\n",
        "      empl.append(s)\n",
        "\n",
        "    for j in data[i][\"confirmed_resumes\"]:\n",
        "      s = ''\n",
        "      if (\"about\" in j):\n",
        "        if j['about'] is not None:\n",
        "          s+=j['about'] + \" \"\n",
        "      if ('experienceItem' in j):\n",
        "        for k in j['experienceItem']:\n",
        "          if ('description' in k):\n",
        "            if k['description'] is not None:\n",
        "              s += k['description'] + \" \"\n",
        "      empl.append(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SV0SKFJ1EPZZ"
      },
      "outputs": [],
      "source": [
        "def preprocess(arr):\n",
        "\n",
        "  for i in range(len(arr)):\n",
        "\n",
        "    arr[i] = re.sub(r'[.,!?\\'\\\"();:]', '', arr[i])\n",
        "    text = arr[i]\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('russian'))\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_tokens]\n",
        "    arr[i] = stemmed_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KxHMPYnelpjY"
      },
      "outputs": [],
      "source": [
        "preprocess(empl)\n",
        "preprocess(descriptions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OGF8qt7Yovbo"
      },
      "outputs": [],
      "source": [
        "new_array = []\n",
        "array_answer = []\n",
        "\n",
        "\n",
        "for i in range(len(empl)):\n",
        "  tmp = empl[i]\n",
        "  k = 0\n",
        "  while(k+100 < len(tmp)):\n",
        "    new_array.append(tmp[k:k+100])\n",
        "    array_answer.append(tmp[k+100])\n",
        "    k+=1\n",
        "\n",
        "for i in range(len(descriptions)):\n",
        "  tmp = empl[i]\n",
        "  k = 0\n",
        "  while(k+100 < len(tmp)):\n",
        "    new_array.append(tmp[k:k+100])\n",
        "    array_answer.append(tmp[k+100])\n",
        "    k+=1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens\n",
        "\n",
        "sentences = new_array + array_answer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Word index:\", word_index)\n",
        "\n",
        "print(\"Encoded sequences:\")\n",
        "result = (padded_sequences - padded_sequences.mean()) / padded_sequences.std()\n"
      ],
      "metadata": {
        "id": "hv6KgiTXYcMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data_for_train_test\n",
        "\n",
        "result1 = result[:len(result)-108496]\n",
        "result2 = result[len(result)-108496:]\n",
        "res = []\n",
        "for i in result2:\n",
        "  res.append(i[0])\n",
        "\n",
        "result2 = res\n",
        "\n",
        "X = result1\n",
        "y = result2\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(X, y, random_state=43, test_size=0.2)"
      ],
      "metadata": {
        "id": "c-VRjKJrkHmC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EarlyStopping\n",
        "\n",
        "\n",
        "device = 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class EarlyStopping():\n",
        "\n",
        "  def __init__(self, patience=5, min_delta=1e-3, restore_best_weights=True):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.restore_best_weights = restore_best_weights\n",
        "    self.best_model = None\n",
        "    self.best_loss = None\n",
        "    self.counter = 0\n",
        "    self.status = \"\"\n",
        "\n",
        "\n",
        "  def __call__(self, model, val_loss):\n",
        "    if self.best_loss == None:\n",
        "      self.best_loss = val_loss\n",
        "      self.best_model = copy.deepcopy(model)\n",
        "    elif self.best_loss - val_loss > self.min_delta:\n",
        "      self.best_loss = val_loss\n",
        "      self.counter = 0\n",
        "      self.best_model.load_state_dict(model.state_dict())\n",
        "    elif self.best_loss - val_loss < self.min_delta:\n",
        "      self.counter += 1\n",
        "      if self.counter >= self.patience:\n",
        "        self.status = f\"Stopped on {self.counter}\"\n",
        "        if self.restore_best_weights:\n",
        "          model.load_state_dict(self.best_model.state_dict())\n",
        "        return True\n",
        "\n",
        "\n",
        "    self.status = f\"{self.counter}/{self.patience}\"\n",
        "    return False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0QRez5IlXUu",
        "outputId": "eaa59192-31d1-4321-f2f1-2f546e97bc95"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        h0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.num_layers, self.hidden_size).to(x.device)\n",
        "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
        "\n",
        "        return out, hn, cn\n",
        "\n",
        "\n",
        "X_train = torch.Tensor(X_train).float()\n",
        "y_train = torch.Tensor(y_train).float()\n",
        "\n",
        "X_validation = torch.Tensor(X_validation).float().to(device)\n",
        "y_validation = torch.Tensor(y_validation).float().to(device)\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "dataset_train = TensorDataset(X_train, y_train)\n",
        "dataloader_train = DataLoader(dataset_train,\\\n",
        "  batch_size=batch_size)\n",
        "\n",
        "dataset_test = TensorDataset(X_validation, y_validation)\n",
        "dataloader_test = DataLoader(dataset_test,\\\n",
        "  batch_size=batch_size)\n",
        "\n",
        "model = Net(X.shape[1], 4, 2, 1).to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "es = EarlyStopping()\n",
        "\n",
        "epoch = 0\n",
        "done = False\n",
        "while epoch < 100 and not done:\n",
        "  epoch += 1\n",
        "  steps = list(enumerate(dataloader_train))\n",
        "  pbar = tqdm.tqdm(steps)\n",
        "  model.train()\n",
        "  for i, (x_batch, y_batch) in pbar:\n",
        "    y_batch_pred = model(x_batch.to(device))[0].flatten()\n",
        "    loss = loss_fn(y_batch_pred[0], y_batch.to(device))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
        "    if i == len(steps)-1:\n",
        "      model.eval()\n",
        "      pred = model(X_validation)[0].flatten()\n",
        "      vloss = loss_fn(pred[0], y_validation)\n",
        "      if es(model,vloss): done = True\n",
        "      pbar.set_description(f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
        "    else:\n",
        "      pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
      ],
      "metadata": {
        "id": "dmqH5PielgOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = Net(X.shape[1], 4, 2, 1)\n",
        "torch.save(network.state_dict(), '/content/model')"
      ],
      "metadata": {
        "id": "pdpA-PXXWJGB"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predict and convert\n",
        "\n",
        "def predict(model, input):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input)\n",
        "    return predictions\n",
        "\n",
        "def convert_to_2D(arr):\n",
        "\n",
        "  arr1 = np.zeros((1, len(arr)))\n",
        "  for i in range(0, len(arr)):\n",
        "      arr1[0][i] = arr[i]\n",
        "  return arr1"
      ],
      "metadata": {
        "id": "ZuBoMhm6owGZ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#тесты\n",
        "\n",
        "vacancy_test = data[len(data)-1]['vacancy']['description']\n",
        "vacancy_test = [vacancy_test]\n",
        "preprocess(vacancy_test)\n",
        "failed = data[len(data)-1][\"failed_resumes\"]\n",
        "good = data[len(data)-1][\"confirmed_resumes\"]\n",
        "\n",
        "failed_array = []\n",
        "for j in failed:\n",
        "      s = ''\n",
        "      if (\"about\" in j):\n",
        "        if j['about'] is not None:\n",
        "          s+=j['about'] + \" \"\n",
        "      if ('experienceItem' in j):\n",
        "        for k in j['experienceItem']:\n",
        "          if ('description' in k):\n",
        "            if k['description'] is not None:\n",
        "              s += k['description'] + \" \"\n",
        "      failed_array.append(s)\n",
        "\n",
        "\n",
        "good_array = []\n",
        "for j in good:\n",
        "      s = ''\n",
        "      if (\"about\" in j):\n",
        "        if j['about'] is not None:\n",
        "          s+=j['about'] + \" \"\n",
        "      if ('experienceItem' in j):\n",
        "        for k in j['experienceItem']:\n",
        "          if ('description' in k):\n",
        "            if k['description'] is not None:\n",
        "              s += k['description'] + \" \"\n",
        "      good_array.append(s)\n",
        "\n",
        "\n",
        "preprocess(failed_array)\n",
        "preprocess(good_array)"
      ],
      "metadata": {
        "id": "QQFXSk8qGyBV"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokens\n",
        "\n",
        "sentences = vacancy_test + good_array + failed_array\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "max_seq_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(\"Word index:\", word_index)\n",
        "\n",
        "\n",
        "print(\"Encoded sequences:\")\n",
        "result = (padded_sequences - padded_sequences.mean()) / padded_sequences.std()\n",
        "\n",
        "\n",
        "vac = result[0]\n",
        "good = result[1:7]\n",
        "bad = result[7:]"
      ],
      "metadata": {
        "id": "ToDkoCPPODBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out, hidden_states_vac, cell_states = predict(model, torch.Tensor(convert_to_2D(vac[:100])).float())"
      ],
      "metadata": {
        "id": "BkB7JIW3Pz8k"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in good:\n",
        "  out, hidden_states_good, cell_states = predict(model, torch.Tensor(convert_to_2D(i[:100])).float())\n",
        "  print('good', ds.cosine(hidden_states_vac[0], hidden_states_good[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pq1ZLIxQyX_",
        "outputId": "b0671176-7ac1-4764-b3ce-ece76263084a"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good 1.9806910753250122\n",
            "good 0.9997750626207562\n",
            "good 0.0040604472160339355\n",
            "good 1.9959312081336975\n",
            "good 0.01819556951522827\n",
            "good 1.0010957774939016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in bad:\n",
        "  out, hidden_states_bad, cell_states = predict(model, torch.Tensor(convert_to_2D(i[:100])).float())\n",
        "  print('bad', ds.cosine(hidden_states_vac[0], hidden_states_bad[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8x2I_gLHRVMi",
        "outputId": "65fd682d-34eb-48df-b3b0-9728887a94a9"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bad 1.23482283949852\n",
            "bad 1.9955083131790161\n",
            "bad 1.023731030523777\n",
            "bad 0.0040604472160339355\n",
            "bad 1.011991967447102\n",
            "bad 1.995939314365387\n",
            "bad 0.004060506820678711\n",
            "bad 0.17646640539169312\n",
            "bad 0.0040604472160339355\n",
            "bad 1.995939552783966\n",
            "bad 0.0040604472160339355\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}